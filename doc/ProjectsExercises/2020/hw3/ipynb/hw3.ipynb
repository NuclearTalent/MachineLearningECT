{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: Fourth and fifth days: Homework set 3 -->\n",
    "# Fourth and fifth days: Homework set 3\n",
    "<!-- dom:AUTHOR: Data Analysis and Machine Learning -->\n",
    "<!-- Author: -->  \n",
    "**Data Analysis and Machine Learning**\n",
    "\n",
    "Date: **Jul 3, 2020**\n",
    "\n",
    "## Day four and five exercises\n",
    "\n",
    "The exercises here are somewhat longer and we expect to use at least two days on them.\n",
    "\n",
    "### Exercise 1, Bias-Variance tradeoff and Bootstrap\n",
    "\n",
    "This exercise is a continuation of exercise 2 from the second homework set.\n",
    "In that exercise we computed the MSE-score for the training\n",
    "data and the test data as functions of the complexity of a polynomial,\n",
    "that is the degree of a given polynomial.\n",
    "\n",
    "One of the aims of that exercise was \n",
    "to reproduce Figure 2.11 of [Hastie et al](https://github.com/CompPhysics/MLErasmus/blob/master/doc/Textbooks/elementsstat.pdf).\n",
    "\n",
    "Our data is defined by $x\\in [-3,3]$ with a total of for example $100$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed()\n",
    "n = 100\n",
    "maxdegree = 14\n",
    "# Make data set.\n",
    "x = np.linspace(-3, 3, n).reshape(-1, 1)\n",
    "y = np.exp(-x**2) + 1.5 * np.exp(-(x-2)**2)+ np.random.normal(0, 0.1, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $y$ is the function we want to fit with a given polynomial.\n",
    "\n",
    "### Part (1a) Proving the bias-variance tradeoff\n",
    "\n",
    "Consider a\n",
    "dataset $\\mathcal{L}$ consisting of the data\n",
    "$\\mathbf{X}_\\mathcal{L}=\\{(y_j, \\boldsymbol{x}_j), j=0\\ldots n-1\\}$.\n",
    "\n",
    "Let us assume that the true data is generated from a noisy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{y}=f(\\boldsymbol{x}) + \\boldsymbol{\\epsilon}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\epsilon$ is normally distributed with mean zero and standard\n",
    "deviation $\\sigma^2$.\n",
    "\n",
    "In our derivation of the ordinary least squares method we defined then\n",
    "an approximation to the function $f$ in terms of the parameters\n",
    "$\\boldsymbol{\\beta}$ and the design matrix $\\boldsymbol{X}$ which embody our model,\n",
    "that is $\\boldsymbol{\\tilde{y}}=\\boldsymbol{X}\\boldsymbol{\\beta}$.\n",
    "\n",
    "The parameters $\\boldsymbol{\\beta}$ are in turn found by optimizing the means\n",
    "squared error via the so-called cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "C(\\boldsymbol{X},\\boldsymbol{\\beta}) =\\frac{1}{n}\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2=\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that you can rewrite  this as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbb{E}\\left[(\\boldsymbol{y}-\\boldsymbol{\\tilde{y}})^2\\right]=\\frac{1}{n}\\sum_i(f_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2+\\frac{1}{n}\\sum_i(\\tilde{y}_i-\\mathbb{E}\\left[\\boldsymbol{\\tilde{y}}\\right])^2+\\sigma^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain what the terms mean, which one is the bias and which one is\n",
    "the variance and discuss their interpretations.\n",
    "\n",
    "\n",
    "### Part (1b) Adding Bootstrap and Bias-Variance Tradeoff\n",
    "\n",
    "Add now bootstrapping as discussed in the [Regression lectures](https://compphysics.github.io/MachineLearningECT/doc/pub/Day1/html/Day1.html) (scroll down to the bias-variance code).\n",
    "Add also the expressions for the bias and the variance as discussed above.\n",
    "\n",
    "Discuss the bias and variance tradeoff as function\n",
    "of your model complexity (the degree of the polynomial) and the number\n",
    "of data points, and possibly also your training and test data.\n",
    "\n",
    "Try to make a figure similar to Fig. 2.11 of Hastie et al. You should include an analysis of the bias and variance for the test results. Figure 2.11 displays only the test and training MSEs while indicating regions of low/high bias and variance. You will most likely not get an\n",
    "equally smooth curve! Note also that when you calculate the bias, in all applications you don't know the function values $f_i$. You would hence replace them with the actual data points $y_i$.\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 2, Linear Regression for  a two-dimensional function\n",
    "\n",
    "This is a longer  exercise and the aim is to study in more detail various\n",
    "regression methods, including the Ordinary Least Squares (OLS) method,\n",
    "Ridge regression and finally Lasso regression.\n",
    "The methods are in turn combined with resampling techniques.\n",
    "\n",
    "We will study how to fit polynomials to a specific\n",
    "two-dimensional function called [Franke's\n",
    "function](http://www.dtic.mil/dtic/tr/fulltext/u2/a081688.pdf).  This\n",
    "is a function which has been widely used when testing various\n",
    "interpolation and fitting algorithms. Furthermore, after having\n",
    "established the model and the method, we will employ resamling\n",
    "like the bootstrap from the previous exercise in order to perform a\n",
    "proper assessment of our models. We will also study in detail the\n",
    "so-called Bias-Variance trade off.\n",
    "\n",
    "\n",
    "The Franke function, which is a weighted sum of four exponentials  reads as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "f(x,y) &= \\frac{3}{4}\\exp{\\left(-\\frac{(9x-2)^2}{4} - \\frac{(9y-2)^2}{4}\\right)}+\\frac{3}{4}\\exp{\\left(-\\frac{(9x+1)^2}{49}- \\frac{(9y+1)}{10}\\right)} \\\\\n",
    "&+\\frac{1}{2}\\exp{\\left(-\\frac{(9x-7)^2}{4} - \\frac{(9y-3)^2}{4}\\right)} -\\frac{1}{5}\\exp{\\left(-(9x-4)^2 - (9y-7)^2\\right) }.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function will be defined for $x,y\\in [0,1]$.  Our first step will\n",
    "be to perform an OLS regression analysis of this function, trying out\n",
    "a polynomial fit with an $x$ and $y$ dependence of the form $[x, y,\n",
    "x^2, y^2, xy, \\dots]$. We will also include cross-validation (or bootstrap) as\n",
    "resampling technique.  As in homeworks 1 and 2, we can use a uniform\n",
    "distribution to set up the arrays of values for $x$ and $y$, or as in\n",
    "the example below just a set of fixed \n",
    "values for $x$ and $y$ with a given step\n",
    "size.  We will fit a\n",
    "function (for example a polynomial) of $x$ and $y$.  Thereafter we\n",
    "will repeat much of the same procedure using the Ridge and Lasso\n",
    "regression methods, introducing thus a dependence on the bias\n",
    "(penalty) $\\lambda$.\n",
    "\n",
    "Finally we are going to use (real) digital terrain data and try to\n",
    "reproduce these data using the same methods. We will also try to go\n",
    "beyond the second-order polynomials metioned above and explore \n",
    "which polynomial fits the data best.\n",
    "\n",
    "\n",
    "The Python fucntion for the Franke function is included here (it performs also a three-dimensional plot of it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "from random import random, seed\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(0, 1, 0.05)\n",
    "y = np.arange(0, 1, 0.05)\n",
    "x, y = np.meshgrid(x,y)\n",
    "\n",
    "\n",
    "def FrankeFunction(x,y):\n",
    "    term1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "    term2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "    term3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "    term4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "    return term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "z = FrankeFunction(x, y)\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(x, y, z, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(-0.10, 1.40)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2a) Ordinary Least Square on the Franke function  with resampling\n",
    "\n",
    "We will generate our own dataset for a function\n",
    "$\\mathrm{FrankeFunction}(x,y)$ with $x,y \\in [0,1]$. The function\n",
    "$f(x,y)$ is the Franke function. You should explore also the addition\n",
    "an added stochastic noise to this function using the normal\n",
    "distribution $\\cal{N}(0,1)$.\n",
    "\n",
    "Write your own code (using either a matrix inversion or a singular\n",
    "value decomposition from e.g., **numpy** ) or use your code from\n",
    "homeworks 1 and 2 and perform a standard least square regression\n",
    "analysis using polynomials in $x$ and $y$ up to fifth order. You can use **scikit-learn** as well.\n",
    "\n",
    "\n",
    "Evaluate the Mean Squared error (MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "MSE(\\hat{y},\\hat{\\tilde{y}}) = \\frac{1}{n}\n",
    "\\sum_{i=0}^{n-1}(y_i-\\tilde{y}_i)^2,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the $R^2$ score function.  If $\\tilde{\\hat{y}}_i$ is the predicted\n",
    "value of the $i-th$ sample and $y_i$ is the corresponding true value,\n",
    "then the score $R^2$ is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "R^2(\\hat{y}, \\tilde{\\hat{y}}) = 1 - \\frac{\\sum_{i=0}^{n - 1} (y_i - \\tilde{y}_i)^2}{\\sum_{i=0}^{n - 1} (y_i - \\bar{y})^2},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where we have defined the mean value  of $\\hat{y}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\bar{y} =  \\frac{1}{n} \\sum_{i=0}^{n - 1} y_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the design matrix, the following code can be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FrankeFunction(x,y):\n",
    "\tterm1 = 0.75*np.exp(-(0.25*(9*x-2)**2) - 0.25*((9*y-2)**2))\n",
    "\tterm2 = 0.75*np.exp(-((9*x+1)**2)/49.0 - 0.1*(9*y+1))\n",
    "\tterm3 = 0.5*np.exp(-(9*x-7)**2/4.0 - 0.25*((9*y-3)**2))\n",
    "\tterm4 = -0.2*np.exp(-(9*x-4)**2 - (9*y-7)**2)\n",
    "\treturn term1 + term2 + term3 + term4\n",
    "\n",
    "\n",
    "def create_X(x, y, n ):\n",
    "\tif len(x.shape) > 1:\n",
    "\t\tx = np.ravel(x)\n",
    "\t\ty = np.ravel(y)\n",
    "\n",
    "\tN = len(x)\n",
    "\tl = int((n+1)*(n+2)/2)\t\t# Number of elements in beta\n",
    "\tX = np.ones((N,l))\n",
    "\n",
    "\tfor i in range(1,n+1):\n",
    "\t\tq = int((i)*(i+1)/2)\n",
    "\t\tfor k in range(i+1):\n",
    "\t\t\tX[:,q+k] = (x**(i-k))*(y**k)\n",
    "\n",
    "\treturn X\n",
    "\n",
    "\n",
    "# Making meshgrid of datapoints and compute Franke's function\n",
    "n = 5\n",
    "N = 1000\n",
    "x = np.sort(np.random.uniform(0, 1, N))\n",
    "y = np.sort(np.random.uniform(0, 1, N))\n",
    "z = FrankeFunction(x, y)\n",
    "X = create_X(x, y, n=n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (2b) Resampling techniques, adding more complexity\n",
    "\n",
    "Perform a resampling of the data where you split the data in training\n",
    "data and test data. Here you can write your own function or use the\n",
    "function for splitting training data provided by **Scikit-Learn**.\n",
    "This function is called $train\\_test\\_split$.   You should also renormalize your data.\n",
    "\n",
    "It is normal in essentially all Machine Learning studies to split the\n",
    "data in a training set and a test set (sometimes also an additional\n",
    "validation set).  There\n",
    "is no explicit recipe for how much data should be included as training\n",
    "data and say test data.  An accepted rule of thumb is to use\n",
    "approximately $2/3$ to $4/5$ of the data as training data.\n",
    "\n",
    "Use then the _bootstrap code you developed in the previous exercise to resample your data\n",
    "and evaluate again the MSE function resulting\n",
    "from the test data. \n",
    "\n",
    "\n",
    "### Part (2c): Bias-variance tradeoff\n",
    "\n",
    "With a code which does OLS and includes bootstrap\n",
    "we will now discuss the bias-variance tradeoff in the context of\n",
    "continuous predictions such as regression. However, many of the\n",
    "intuitions and ideas discussed here also carry over to classification\n",
    "tasks and basically all Machine Learning algorithms. \n",
    "\n",
    "Use the code from exercise 1 above and implement the bootstrap\n",
    "resampling and perform a bias-variance tradeoff analysis like you did\n",
    "in exercise 1.\n",
    "\n",
    "### Part (2d): Ridge Regression on the Franke function  with resampling\n",
    "\n",
    "Write your own code for the Ridge method, either using matrix\n",
    "inversion or the singular value decomposition ir use **scikit-learn** \n",
    "Perform the same analysis as in the\n",
    "previous three steps (for the same polynomials and include resampling\n",
    "techniques) but now for different values of $\\lambda$. Compare and\n",
    "analyze your results with those obtained in parts 2a-2c). Study the\n",
    "dependence on $\\lambda$.\n",
    "\n",
    "Study also the bias-variance tradeoff as function of various values of\n",
    "the parameter $\\lambda$. Comment your results. \n",
    "\n",
    "### Part (2e): Lasso Regression on the Franke function  with resampling\n",
    "\n",
    "This part is essentially a repeat of the previous ones, but now\n",
    "with Lasso regression. Write either your own code or\n",
    "use the functionalities of **Scikit-Learn** (recommended). \n",
    "Give a\n",
    "critical discussion of the three methods and a judgement of which\n",
    "model fits the data best."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
